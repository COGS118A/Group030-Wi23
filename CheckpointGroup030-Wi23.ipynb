{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Project Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Andy Chow\n",
    "- Naomi Chin\n",
    "- Andrew Lona\n",
    "- Jiaqi Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "This project aims to make a model that can find the underlying factors that contribute to a person's financial health and how different demographics may affect the financial health. We will be using the UCI bank marketing data set to obtain demographic information. We will be using Support Vector Machine model and K Nearest Neighbors multiclass classification to predict bank account balances, with the possibly of using Principle Component Analysis to determine the most significant contributors to financial status. We will be using confusion matrix, F1 score, ROC-AUC score, learning curve, and AIC and BIC score comparison to evaluate model performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Making money and having a large bank account balance is a goal that many strive for. But what factors contribute to how much money people have in their accounts? There's the obvious job type and age that typically have an impact on salary and thus bank account balnce, but what other attributes have an influence, and to what degree?\n",
    "\n",
    "There have been studies on how different demographics affect salary or bank account balance. For example, an article by ValuePenguin breaks down how income and age affect balances. Unsuprisingly, as income increases, the average balance increases. In addition, balance increases as age increases up to the 65-74 age range, but after 75+ years, the average balance decreases <a name=\"moon\"></a>[<sup>[1]</sup>](#moonnote).\n",
    "There are also a multitude of studies that have looked at how different personal attributes affect earnings. While earnings are not exactly bank account balance, we have already seen how income affect the balance. A report by Social Security found that men with bachelor's degrees earn about \\\\$900,000 more during their lifetime than men with only high school diplomas. For women, the difference is about \\\\$630,000. The report then took into account certain socio-demographic variables that could influence earnings; after recalculations, men and women with bachelor's  degrees earn \\\\$655,000 and \\\\$450,000, respectively, more than their high school graduate counterparts <a name=\"social\"></a>[<sup>[2]</sup>](#socialnote). Further, Social Security looks into how savings are affected by marital status. It was found that married people were much more likely to have an individual retirement account (IRA) or defined contribution (DC). This likely means that married people are better at saving money due to multiple reasons. Cost sharing, long term commitment, and future-focused behavior may be contributers to this behavior <a name=\"relationship\"></a>[<sup>[3]</sup>](#relationshipnote).\n",
    "\n",
    "We are going to analyze not only what attributes collectively contribute to bank account balance, but also how much influence each attribute has. The goal of our research and analysis is to find what demographics and lifestyle choices correlate to bank account balances. While the results of this research are intended to be informative knowledge, they could serve as guidance to increase bank account balance. This information could also be used for banks to determine what clients to focus on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we hope to solve is, how well can a person’s demographic information predict their financial health? In this instance, financial health is determined by the balance of a person’s bank account (high is good, low is bad). Although a single bank account balance cannot directly indicate a person’s overall financial health, it is a reasonable assumption that a higher bank account balance correlates to better financial health. Our analysis will first determine which aspect of a person’s demographic information has the most influence on bank account balance through a Principal Component Analysis (PCA). Then we will determine how well a person’s age, job type, marital status, education level, credit in default status, housing loan holder status, and loan holder status contribute towards determining a person’s bank account balance. To predict the balances, we will compare Support Vector Machine (SVM) and K Nearest Neighbors (KNN) models for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data that will be used is the UCI bank marketing data set. It gives information of direct marketing campaigns of a Portuguese banking institution.\n",
    "\n",
    "Data link: https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "- 17 of variables, 45211 observations\n",
    "- each observation consists of one marketing call from a Portuguese banking institution to assess if the bank term deposit would be subscribed (by the client). These outcomes may not always results in subscription, and follow-up calls are sometimes made to the same client.\n",
    "- critical variables are\n",
    "    - balance: at the moment we are acting on assumption, but it should be the current account balance of the client contacted\n",
    "    - independent variables such as (numerical age, job type, marital status, education level, credit in default status, housing loan holder status, and loan holder status)\n",
    "- cleaning/transformations\n",
    "    - the data is pretty spotless in a wrangling sense, albeit most of the categorical variables do have an unknown or nonexistent value which will need to be accounted for.\n",
    "    - we will also need to one-hot encode all categorical IVs such as marital status and job type (not dummy coding).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "\n",
    "We are interested in predicting the bank account balance of individuals given their demographic data provided by the bank. Since the account balance variable is independent from other observations, we can convert the continuous range into categorical bins based on quartile ranges with the addition of \"below 0\". \n",
    "\n",
    "\n",
    "We will first perform a PCA to decorrelate the inputs to analyze the unique contributions from each variable. This will help us to determine what features are the most significant contributors to bank account balance.\n",
    "\n",
    "\n",
    "We will then use a SVM model to conduct the multiple classification. In addition, preliminary observation of the target feature reveals that the mean is significantly smaller than the median, which indicates that there are outliers in the dataset. We will be using a ElasticNet regularization primarily to limit the influence of outliers in the L1 term but the L2 term also helps in reduction of the 17 present features. We will conduct a gradient descent with respect to the $\\alpha$ term of the ElasticNet regularization since we are not completely sure how much to penalize for outliers. We will also apply a simple model using K Nearest Neighbors multiclass classification, and apply a model comparison metric to evaluate performance. We will use a Confusion Matrix and ROC-AUC analysis for model benchmarking to compare the SVM and KNN models.\n",
    "\n",
    "\n",
    "We plan to use Pandas for data preprocessing and the modules provided in the SKLearn library to conduct our model implementation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Since we are dealing with a multiclass problem, in order to complete a general performance evaluation of models, we will convert our classes into multiple one-vs-rest classifications. For each instance, we will take each bank account bin as the positive class and all of the other bins as the negative class. So we will have n binary classifications where n is the number of bank account bins.\n",
    "\n",
    "\n",
    "From there, we can create a confusion matrix for each on-vs-rest classification. The true and predicted classes will be used as inputs and the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) will be the outputs of the confusion matrix. We can then aggregate the TP, TN, FP, and FN from all of the classifications. \n",
    "\n",
    "\n",
    "With the aggregate values, we will calculate the F1 score for each model. The F1 score is chosen because neither false positives nor false negatives are particularly penalizing to the final performance of the model. The F1 score is a function of precision and recall which account for false positives and false negatives, respectively. \n",
    "\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP+FP} \\\\\n",
    "Recall = \\frac{TP}{TP+FN} \\\\\n",
    "F1 = \\frac{2*Precision*Recall}{Precision+Recall}\n",
    "$$\n",
    "\n",
    "\n",
    "We will also be calculating a Receiver Operating Characteristic Area Under the Curve (ROC-AUC) score to compare the SVM and KNN models. The ROC-AUC is a good evaluation of a model because it is a measurement of how well a model can distinguish between positive and negative classes. In our case, how well a model completes the multiclass classification. The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at different classification thresholds. The ROC-AUC score ranges from 0 to 1; the larger the score, the better the performance of the model. \n",
    "\n",
    "\n",
    "We will also use a learning curve to evaluate over or underfitting of the model. Finally we will include an Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) score comparison to compare models. These scores measure the relative quality of a model for a given dataset. The scores are calculated with the number of model parameters (k), the maximum value of the likelihood function (L), and the number of observations in the data. \n",
    "\n",
    "\n",
    "$$\n",
    "AIC = 2k-2ln(L) \\\\\n",
    "BIC = kln(n)-2ln(L)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "NEW SECTION!\n",
    "\n",
    "Please show any preliminary results you have managed to obtain.\n",
    "\n",
    "Examples would include:\n",
    "- Analyzing the suitability of a dataset or alogrithm for prediction/solving your problem \n",
    "- Performing feature selection or hand-designing features from the raw data. Describe the features available/created and/or show the code for selection/creation\n",
    "- Showing the performance of a base model/hyper-parameter setting.  Solve the task with one \"default\" algorithm and characterize the performance level of that base model.\n",
    "- Learning curves or validation curves for a particular model\n",
    "- Tables/graphs showing the performance of different models/hyper-parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bank = pd.read_csv('data/bank-full.csv', sep=';')\n",
    "bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of unknown values in the data\n",
    "print('job:', bank[bank.job == 'unknown'].shape[0])\n",
    "print('education:', bank[bank.education == 'unknown'].shape[0])\n",
    "print('contact:', bank[bank.contact == 'unknown'].shape[0])\n",
    "print('poutcome:', bank[bank.poutcome == 'unknown'].shape[0])\n",
    "print('pdays:', bank[bank.pdays == -1].shape[0])  # -1 means client was not previously contacted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After looking at the data, we have decided to remove the columns 'contact', 'day', 'pdays', 'poutcome'.\n",
    "We also remove the columns 'duration' since it is heavily correlated with 'y'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bank = bank.drop(['contact', 'day', 'pdays', 'poutcome', 'duration'], axis=1)\n",
    "bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(bank['balance'].describe())\n",
    "sns.histplot(data=bank, x='balance', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As the above histplot shows, the distribution of balance is very heavily right skewed, with many cluster around 0. This is not useful. Therefore, we will take the log of balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bank_poslog = np.array(bank[bank.balance > 0].balance)\n",
    "bank_poslog = np.log(bank_poslog)\n",
    "bank_neglog = np.array(bank[bank.balance < 0].balance)\n",
    "bank_neglog = np.log(-bank_neglog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "# plot of logged positive and negative balances\n",
    "sns.histplot(data=bank_poslog, color='blue', bins=30, ax=ax1)\n",
    "ax1.title.set_text('Histplot of Logged Positive Balance')\n",
    "sns.histplot(data=bank_neglog, color = 'red', bins=30, ax=ax2)\n",
    "ax2.title.set_text('Histplot of Logged Negative Balance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bank['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.histplot(data=bank, x='age', bins=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The distribution of age is fairly normal. No processing is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "dummy_list = []\n",
    "one_hot_list = []\n",
    "# check to see what vars can be dummied\n",
    "for column in bank.nunique().keys():\n",
    "    unique_value = bank.nunique()[column]\n",
    "    if unique_value == 2:\n",
    "        dummy_list.append(column)\n",
    "    elif unique_value > 2 and (str(bank[column].dtype)) != 'int64':\n",
    "        one_hot_list.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy_dict = {}\n",
    "# Dummy encoding\n",
    "# they all have yes or no, so we can do the same for all\n",
    "for column in dummy_list:\n",
    "    dummy_dict[column] = bank[column].eq('yes').mul(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "one_hot_bank = bank[one_hot_list]\n",
    "encoder.fit(one_hot_bank)\n",
    "# Following line is used for validation\n",
    "# encoder.inverse_transform(one_hot_bank)\n",
    "one_hot_bank = encoder.transform(one_hot_bank).toarray()\n",
    "one_hot_labels = encoder.categories_\n",
    "\n",
    "counter = 0\n",
    "bank_dict = {}\n",
    "for i in range(0, len(one_hot_labels)):\n",
    "    for column in one_hot_labels[i].tolist():\n",
    "        bank_dict[column] = one_hot_bank[:,counter]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now creating new df\n",
    "bank_dict.update(dummy_dict)\n",
    "bank_coded = pd.DataFrame(bank_dict)\n",
    "bank_coded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # constant that represents 0\n",
    "# zero_rep = -3\n",
    "#\n",
    "# bank_poslog = bank.balance\n",
    "# bank_poslog[bank_poslog < 0] = 0  # set all negative numbers to 0\n",
    "# bank_poslog += np.e ** zero_rep   # add a small constant to avoid ln(0)\n",
    "# bank_poslog = np.log(bank_poslog)\n",
    "#\n",
    "# bank_neglog = -bank.balance\n",
    "# bank_neglog[bank_neglog < 0] = 0  # set all negative numbers to 0\n",
    "# bank_neglog += np.e ** zero_rep   # add a small constant to avoid ln(0)\n",
    "# bank_neglog = np.log(bank_neglog)\n",
    "#\n",
    "# bank_coded['pos_balance'] = bank_poslog\n",
    "# bank_coded['neg_balance'] = bank_neglog\n",
    "bank_coded['age'] = bank.age\n",
    "bank_coded['balance'] = bank.balance\n",
    "bank_coded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the necessary libraries and modules\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conducting PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA processing\n",
    "\n",
    "# Preprocess the dataframe by transforming it to the origin with the standard scaler\n",
    "scaled_df = StandardScaler().fit_transform(bank_coded)\n",
    "\n",
    "# fit the PCA function\n",
    "pca = PCA()\n",
    "pca.fit(scaled_df)\n",
    "pca_data = pca.transform(scaled_df)\n",
    "\n",
    "# return the principal components\n",
    "num_features = bank_coded.shape[1]\n",
    "\n",
    "percent_variance = np.round(pca.explained_variance_ratio_ * num_features, decimals = 1)\n",
    "labels = ['PC' + str(x) for x in range(1, len(percent_variance)+1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Scree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree Plot\n",
    "plt.bar(x=range(1,len(percent_variance)+1), height = percent_variance, tick_label = labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('PCA Scree Plot')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SVM (WIP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LearningCurveDisplay, learning_curve\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, f1_score, confusion_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create Quantile Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Pull Y variable out which is balance\n",
    "X = scaled_df[:,0:35]\n",
    "df_y = pd.DataFrame(scaled_df[:,35])\n",
    "quantile_1, quantile_2, quantile_3 = df_y.quantile([0.25,0.50,0.75])[0].tolist()\n",
    "df_y[0] = df_y[0].apply(lambda x: np.nan if pd.isna(x) else (1 if  x < quantile_1 else (2 if x < quantile_2 else (3 if x < quantile_3 else 4))))\n",
    "y = df_y[0].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Conduct SVM Classification \n",
    "(Do NOT run this cell, it currently takes forever, we are testing ways to decrease training time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf.fit(X_train, y_train)\n",
    "GridSearchCV(estimator=SVC(),\n",
    "             param_grid={'C': [1, 10, 100, 1000], 'kernel': ('rbf', 'linear', 'polynomial')})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Plotting Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "predictions = clf.best_estimator_.predict(X_test)\n",
    "cm = confusion_matrix(y_test, predictions, labels=clf.best_estimator_.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                             display_labels=clf.best_estimator_.classes_)\n",
    "disp.plot()\n",
    "plt.xticks(rotation = -45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2 Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_abs, train_scores, test_scores = learning_curve(svc, X_train, y_train, train_sizes=[0.3, 0.6, 0.9])\n",
    "\n",
    "for train_size, cv_train_scores, cv_test_scores in zip(train_size_abs, train_scores, test_scores):\n",
    "    print(f\"{train_size} samples were used to train the model\")\n",
    "    print(f\"The average train accuracy is {cv_train_scores.mean():.2f}\")\n",
    "    print(f\"The average test accuracy is {cv_test_scores.mean():.2f}\")\n",
    "\n",
    "display = LearningCurveDisplay(train_sizes=[0.3, 0.6, 0.9], train_scores=train_scores, test_scores=test_scores, score_name=\"Score\")\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "\n",
    "# Define and Fit your Model\n",
    "# C_param_vals = [1, 10, 100, 1000]\n",
    "# kernels = ['rbf', 'linear', 'polynomial']\n",
    "#\n",
    "# for val, k in zip(C_param_vals, kernels):\n",
    "#     svm_mod = OneVsRestClassifier(SVC(C = val, kernel = k, decision_function_shape='ovr'))\n",
    "#     svm_mod.fit(X_train, y_train)\n",
    "#     svm_pred = svm_mod.predict(X_test)\n",
    "#     svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "#     svm_f1 = f1_score(y_test, svm_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main concern with prediction with real life data is the accuracies of our models. We are uncertain about the predictive power of the models and whether these models should be used to help with future marketing. One can misuse the models trained and infer possibly inaccurate conclusions on bank marketing.\n",
    "\n",
    "Furthermore, the data used are from a Portuguese banking institution and may not be similar with maketing data of banking institutions of other countries. The marketing data of other banking institutions in Portugal might also differ. The model created might not be accurate to predict marketing results of other banking institutions.\n",
    "\n",
    "Therefore, We are careful to disclose that the models we will generate are for reference only and we by no means guarantee they are accurate in predicting the market reception on banking. One needs to be cautious using the models trianed to predict patterns, generalize, and infer conclusions on bank marketing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We expect to make it to as many meetings as possible. If you cannot make it to a meeting due to scheduling conflict, notify as soon as possible (ideally 2 days before), so that other teammates can find a way to resolve the conflict.\n",
    "* We expect everyone to share their ideas, thoughts and concerns on the projects. We want everyone to have an oppotunity to express their ideas. And it also helps to better evaluate the scope of the project.\n",
    "* We expect everyone to share the workload of the project and make consistant updates to the project. This can help us guage how everyone is doing. If a person is falling behind, other teammates can have time to assist and make changes to the project.\n",
    "* We expect everyone to handle conflict in ideas peacefully. Teammates should listen to both sides and come to an agreement without escalating the conflict.\n",
    "* If a teammate is struggling and have difficulties on finishing their task, they should inform other teammates on their problems. Teammates are expected to provide assistance to one other and ensure everyone is on pace.\n",
    "* All members will be agree on proposed changes before making any final changes to any parts of the project.\n",
    "* If a member cannot make a deadline, other group members are expected to help to the best of their abilities without taking over the workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/17(Fri)  |  5 PM   | Brainstorm topics/questions | Brainstorm topics/questions |\n",
    "| 2/22(Wed)  | 7:30 PM | Do background research on topic | Discuss ideal dataset(s) and ethics; finish project proposal |\n",
    "| 2/25(Sat)  |  3 PM   | NA | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part |\n",
    "| 2/28(Tue)  | 7:30 PM | Import & Wrangle Data ,do some EDA | Review/Edit wrangling/EDA; Discuss Analysis Plan |\n",
    "| 3/8(Wed)   | 7:30 PM | Finalize wrangling/EDA; Begin programming for project | Discuss/edit project code; Complete project |\n",
    "| 3/14(Tue)  | 7:30 PM | Complete analysis; Draft results/conclusion/discussion | Discuss/edit full project |\n",
    "| 3/22(Wed)  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"moonnote\"></a>1.[^](#moon): Moon, Chris (14 Sept 2022) Average U.S. Checking Account Balance: A Demographic Breakdown. *ValuePenguin*. https://www.valuepenguin.com/banking/average-checking-account-balance.<br>\n",
    "<a name=\"socialnote\"></a>2.[^](#social): Social Security. Research, Statistics & Policy Analysis: Education and Lifetime Earnings. https://www.ssa.gov/policy/docs/research-summaries/education-earnings.html#:~:text=Men%20with%20bachelor's%20degrees%20earn,earnings%20than%20high%20school%20graduates.<br>\n",
    "<a name=\"relationshipnote\"></a>3.[^](#relationship): Social Security. Research, Statistics & Policy Analysis: The Relationship Between Retirement Savings and Marital Status Among Young Adults. https://www.ssa.gov/policy/docs/research-summaries/marital-status.html.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
