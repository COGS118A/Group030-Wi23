{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert title here\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Andy Chow\n",
    "- Naomi Chin\n",
    "- Andrew Lona\n",
    "- Jiaqi Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "This project aims to make a model that can find the underlying factors that contribute to a person's financial health and how different demographics may affect the financial health. We will be using the UCI bank marketing data set to obtain demographic information. We will be using Support Vector Machine model and K Nearest Neighbors multiclass classification to predict bank account balances, with the possibly of using Principle Component Analysis to determine the most significant contributors to financial status. We will be using confusion matrix, F1 score, ROC-AUC score, learning curve, and AIC and BIC score comparison to evaluate model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Making money and having a large bank account balance is a goal that many strive for. But what factors contribute to how much money people have in their accounts? There's the obvious job type and age that typically have an impact on salary and thus bank account balnce, but what other attributes have an influence, and to what degree?\n",
    "\n",
    "There have been studies on how different demographics affect salary or bank account balance. For example, an article by ValuePenguin breaks down how income and age affect balances. Unsuprisingly, as income increases, the average balance increases. In addition, balance increases as age increases up to the 65-74 age range, but after 75+ years, the average balance decreases <a name=\"moon\"></a>[<sup>[1]</sup>](#moonnote).\n",
    "There are also a multitude of studies that have looked at how different personal attributes affect earnings. While earnings are not exactly bank account balance, we have already seen how income affect the balance. A report by Social Security found that men with bachelor's degrees earn about \\\\$900,000 more during their lifetime than men with only high school diplomas. For women, the difference is about \\\\$630,000. The report then took into account certain socio-demographic variables that could influence earnings; after recalculations, men and women with bachelor's  degrees earn \\\\$655,000 and \\\\$450,000, respectively, more than their high school graduate counterparts <a name=\"social\"></a>[<sup>[2]</sup>](#socialnote). Further, Social Security looks into how savings are affected by marital status. It was found that married people were much more likely to have an individual retirement account (IRA) or defined contribution (DC). This likely means that married people are better at saving money due to multiple reasons. Cost sharing, long term commitment, and future-focused behavior may be contributers to this behavior <a name=\"relationship\"></a>[<sup>[3]</sup>](#relationshipnote).\n",
    "\n",
    "We are going to analyze not only what attributes collectively contribute to bank account balance, but also how much influence each attribute has. The goal of our research and analysis is to find what demographics and lifestyle choices correlate to bank account balances. While the results of this research are intended to be informative knowledge, they could serve as guidance to increase bank account balance. This information could also be used for banks to determine what clients to focus on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem we hope to solve is, how well can a person’s demographic information predict their financial health? In this instance, financial health is determined by the balance of a person’s bank account (high is good, low is bad). Although a single bank account balance cannot directly indicate a person’s overall financial health, it is a reasonable assumption that a higher bank account balance correlates to better financial health. Our analysis will first determine which aspect of a person’s demographic information has the most influence on bank account balance through a Principal Component Analysis (PCA). Then we will determine how well a person’s age, job type, marital status, education level, credit in default status, housing loan holder status, and loan holder status contribute towards determining a person’s bank account balance. To predict the balances, we will compare Support Vector Machine (SVM) and K Nearest Neighbors (KNN) models for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data that will be used is the UCI bank marketing data set. It gives information of direct marketing campaigns of a Portuguese banking institution.\n",
    "\n",
    "Data link: https://archive.ics.uci.edu/ml/datasets/Bank+Marketing\n",
    "- 17 of variables, 45211 observations\n",
    "- each observation consists of one marketing call from a Portuguese banking institution to assess if the bank term deposit would be subscribed (by the client). These outcomes may not always results in subscription, and follow-up calls are sometimes made to the same client.\n",
    "- critical variables are\n",
    "    - balance: at the moment we are acting on assumption, but it should be the current account balance of the client contacted\n",
    "    - independent variables such as (numerical age, job type, marital status, education level, credit in default status, housing loan holder status, and loan holder status)\n",
    "- cleaning/transformations\n",
    "    - the data is pretty spotless in a wrangling sense, albeit most of the categorical variables do have an unknown or nonexistent value which will need to be accounted for.\n",
    "    - we will also need to one-hot encode all categorical IVs such as marital status and job type (not dummy coding).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We are interested in predicting the bank account balance of individuals given their demographic data provided by the bank. Since the account balance variable is independent from other observations, we can convert the continuous range into categorical bins based on quartile ranges with the addition of \"below 0\".\n",
    "\n",
    "\n",
    "We will first perform a PCA to decorrelate the inputs to analyze the unique contributions from each variable. This will help us to determine what features are the most significant contributors to bank account balance.\n",
    "\n",
    "\n",
    "We will then use a SVM model to conduct the multiple classification. In addition, preliminary observation of the target feature reveals that the mean is significantly smaller than the median, which indicates that there are outliers in the dataset. We will be using a ElasticNet regularization primarily to limit the influence of outliers in the L1 term but the L2 term also helps in reduction of the 17 present features. We will conduct a gradient descent with respect to the $\\alpha$ term of the ElasticNet regularization since we are not completely sure how much to penalize for outliers. We will also apply a simple model using K Nearest Neighbors multiclass classification, and apply a model comparison metric to evaluate performance. We will use a Confusion Matrix and ROC-AUC analysis for model benchmarking to compare the SVM and KNN models.\n",
    "\n",
    "\n",
    "We plan to use Pandas for data preprocessing and the modules provided in the SKLearn library to conduct our model implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Since we are dealing with a multiclass problem, in order to complete a general performance evaluation of models, we will convert our classes into multiple one-vs-rest classifications. For each instance, we will take each bank account bin as the positive class and all of the other bins as the negative class. So we will have n binary classifications where n is the number of bank account bins.\n",
    "\n",
    "\n",
    "From there, we can create a confusion matrix for each on-vs-rest classification. The true and predicted classes will be used as inputs and the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) will be the outputs of the confusion matrix. We can then aggregate the TP, TN, FP, and FN from all of the classifications.\n",
    "\n",
    "\n",
    "With the aggregate values, we will calculate the F1 score for each model. The F1 score is chosen because neither false positives nor false negatives are particularly penalizing to the final performance of the model. The F1 score is a function of precision and recall which account for false positives and false negatives, respectively.\n",
    "\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP+FP} \\\\\n",
    "Recall = \\frac{TP}{TP+FN} \\\\\n",
    "F1 = \\frac{2*Precision*Recall}{Precision+Recall}\n",
    "$$\n",
    "\n",
    "\n",
    "We will also be calculating a Receiver Operating Characteristic Area Under the Curve (ROC-AUC) score to compare the SVM and KNN models. The ROC-AUC is a good evaluation of a model because it is a measurement of how well a model can distinguish between positive and negative classes. In our case, how well a model completes the multiclass classification. The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at different classification thresholds. The ROC-AUC score ranges from 0 to 1; the larger the score, the better the performance of the model.\n",
    "\n",
    "\n",
    "We will also use a learning curve to evaluate over or underfitting of the model. Finally we will include an Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) score comparison to compare models. These scores measure the relative quality of a model for a given dataset. The scores are calculated with the number of model parameters (k), the maximum value of the likelihood function (L), and the number of observations in the data.\n",
    "\n",
    "\n",
    "$$\n",
    "AIC = 2k-2ln(L) \\\\\n",
    "BIC = kln(n)-2ln(L)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Wrangling\n",
    "\n",
    "We will first import the Banking Dataset and the packages required to run our models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bank = pd.read_csv('data/bank-full.csv', sep=';')\n",
    "print(bank.head())\n",
    "\n",
    "# other imports needed for ML and data cleaning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# we can't use f1, or anything conf matrix metrics due to no probabilities\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, make_scorer, roc_auc_score, ConfusionMatrixDisplay, f1_score, confusion_matrix, accuracy_score, classification_report, RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, LearningCurveDisplay, learning_curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we need to check data that allows for unknown values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Number of unknown values in the data\n",
    "print('job:', bank[bank.job == 'unknown'].shape[0])\n",
    "print('education:', bank[bank.education == 'unknown'].shape[0])\n",
    "print('contact:', bank[bank.contact == 'unknown'].shape[0])\n",
    "print('poutcome:', bank[bank.poutcome == 'unknown'].shape[0])\n",
    "print('pdays:', bank[bank.pdays == -1].shape[0])  # -1 means client was not previously contacted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After looking at the data, we have decided to remove the columns 'contact', 'day', 'pdays', 'poutcome'. We also remove the columns 'duration' since it is heavily correlated with 'y'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bank = bank.drop(['contact', 'day', 'pdays', 'poutcome', 'duration'], axis=1)\n",
    "print(bank.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moving onto numerical data. We first checked balance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(bank['balance'].describe())\n",
    "sns.histplot(data=bank, x='balance', bins=50)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the above histplot shows, the distribution of balance is very heavily right skewed, with many cluster around 0. This is not useful. Therefore, we will take the log of balance. We will address how we incorporate it in the encoding section, including how to manage a balance of 0, but for now we would like to visualize the resulting data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bank_poslog = np.array(bank[bank.balance > 0].balance)\n",
    "bank_poslog = np.log(bank_poslog)\n",
    "bank_neglog = np.array(bank[bank.balance < 0].balance)\n",
    "bank_neglog = np.log(-bank_neglog)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "# plot of logged positive and negative balances\n",
    "sns.histplot(data=bank_poslog, color='blue', bins=30, ax=ax1)\n",
    "ax1.title.set_text('Histplot of Logged Positive Balance')\n",
    "sns.histplot(data=bank_neglog, color = 'red', bins=30, ax=ax2)\n",
    "ax2.title.set_text('Histplot of Logged Negative Balance')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The resulting plot is relatively gaussian. Hence we decide to use that to maximize the prediction power of the model.\n",
    "\n",
    "The next data is age."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(bank['age'].describe())\n",
    "sns.histplot(data=bank, x='age', bins=20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The distribution of age is fairly normal. No processing is needed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoding\n",
    "\n",
    "First we will perform dummy encoding to the binary data, and one-hot encoding to categorical data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dummy_list = []\n",
    "one_hot_list = []\n",
    "# check to see what vars can be dummied\n",
    "for column in bank.nunique().keys():\n",
    "    unique_value = bank.nunique()[column]\n",
    "    if unique_value == 2:\n",
    "        dummy_list.append(column)\n",
    "    elif unique_value > 2 and (str(bank[column].dtype)) != 'int64':\n",
    "        one_hot_list.append(column)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dummy_dict = {}\n",
    "# Dummy encoding\n",
    "# they all have yes or no, so we can do the same for all\n",
    "for column in dummy_list:\n",
    "    dummy_dict[column] = bank[column].eq('yes').mul(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "one_hot_bank = bank[one_hot_list]\n",
    "encoder.fit(one_hot_bank)\n",
    "# Following line is used for validation\n",
    "# encoder.inverse_transform(one_hot_bank)\n",
    "one_hot_bank = encoder.transform(one_hot_bank).toarray()\n",
    "one_hot_labels = encoder.categories_\n",
    "\n",
    "counter = 0\n",
    "bank_dict = {}\n",
    "for i in range(0, len(one_hot_labels)):\n",
    "    for column in one_hot_labels[i].tolist():\n",
    "        bank_dict[column] = one_hot_bank[:,counter]\n",
    "        counter += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# now creating new df\n",
    "bank_dict.update(dummy_dict)\n",
    "bank_coded = pd.DataFrame(bank_dict)\n",
    "bank_coded.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we process and add the numerical datas to the encoding."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# constant that represents 0\n",
    "zero_rep = -3\n",
    "\n",
    "bank_log = bank.balance\n",
    "bank_log += np.e ** zero_rep\n",
    "bank_coded['age'] = bank.age\n",
    "bank_coded['balance'] = np.log(bank_log)\n",
    "\n",
    "# bank_poslog = bank.balance\n",
    "# bank_poslog[bank_poslog < 0] = 0  # set all negative numbers to 0\n",
    "# bank_poslog += np.e ** zero_rep   # add a small constant to avoid ln(0)\n",
    "# bank_poslog = np.log(bank_poslog)\n",
    "#\n",
    "# bank_neglog = -bank.balance\n",
    "# bank_neglog[bank_neglog < 0] = 0  # set all negative numbers to 0\n",
    "# bank_neglog += np.e ** zero_rep   # add a small constant to avoid ln(0)\n",
    "# bank_neglog = np.log(bank_neglog)\n",
    "#\n",
    "# bank_coded['pos_balance'] = bank_poslog\n",
    "# bank_coded['neg_balance'] = bank_neglog"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# running continuous vars through standard scalar\n",
    "col_names = ['age']\n",
    "features = bank_coded[col_names]\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "        ('somename', StandardScaler(), ['age'])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "bank_coded['age'] = ct.fit_transform(features)\n",
    "\n",
    "# generating IQR and dropping rows which fall outside\n",
    "Q1, Q3 = bank_coded[\"balance\"].quantile([.25, .75]).tolist()\n",
    "IQR = Q3 - Q1\n",
    "bank_coded = bank_coded.query('(@Q1 - 1.5 * @IQR) <= balance <= (@Q3 + 1.5 * @IQR)')\n",
    "\n",
    "# categorizing balance into quantiles\n",
    "bank_coded['balance'] = pd.qcut(bank_coded['balance'], 4, labels=False)\n",
    "bank_coded.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Principal Component Analysis\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PCA processing\n",
    "\n",
    "# Preprocess the dataframe by transforming it to the origin with the standard scaler\n",
    "scaled_df = StandardScaler().fit_transform(bank_coded)\n",
    "\n",
    "# fit the PCA function\n",
    "pca = PCA()\n",
    "pca.fit(scaled_df)\n",
    "pca_data = pca.transform(scaled_df)\n",
    "\n",
    "# return the principal components\n",
    "num_features = bank_coded.shape[1]\n",
    "\n",
    "percent_variance = np.round(pca.explained_variance_ratio_ * num_features, decimals = 1)\n",
    "labels = ['PC' + str(x) for x in range(1, len(percent_variance)+1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scree Plot\n",
    "plt.bar(x=range(1,len(percent_variance)+1), height = percent_variance, tick_label = labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('PCA Scree Plot')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Models\n",
    "\n",
    "Here we split the data to train-test split, and create a function to score our models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# training and testing splits\n",
    "# Pull Y variable out which is balance\n",
    "X = bank_coded.iloc[:, 0:35].to_numpy()\n",
    "y = bank_coded.iloc[:, 35:36].to_numpy().flatten()\n",
    "\n",
    "# basic T/T/Split modeling due to large size of data\n",
    "X_train_bank, X_test_bank, y_train_bank, y_test_bank = train_test_split(X, y, test_size=0.33, random_state=1, stratify=y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# very useful function I built using a combination of templates from sklearn and other sites\n",
    "\n",
    "def metrics_classification_scorer(y_train, y_test, y_pred, y_proba, show_stats = True, save_auc = False):\n",
    "    \"\"\"\n",
    "    :param y_test: your testing data\n",
    "    :param y_pred: your predictions generated from classifier\n",
    "    :param y_proba: your probabilities generated from classifier\n",
    "    :param show_stats: default is True, shows ROC Curve, and all other Metrics\n",
    "    :param save_auc: saves AUC score if desired (must write to variable)\n",
    "    :return: plots of ROC curve, Conf Matrix Metrics, and AUC Score\n",
    "    \"\"\"\n",
    "\n",
    "    label_binarizer = LabelBinarizer().fit(y_train)\n",
    "    y_onehot_test = label_binarizer.transform(y_test)\n",
    "    # y_onehot_test.shape  # (n_samples, n_classes)\n",
    "\n",
    "    if show_stats:\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_onehot_test.ravel(),\n",
    "            y_proba.ravel(),\n",
    "            name=\"micro-average OvR\",\n",
    "            color=\"darkorange\",\n",
    "        )\n",
    "        plt.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "        plt.axis(\"square\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"Micro-averaged One-vs-Rest\\nReceiver Operating Characteristic\")\n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.25))\n",
    "        plt.show()\n",
    "\n",
    "    micro_roc_auc_ovr = roc_auc_score(\n",
    "        y_test,\n",
    "        y_proba,\n",
    "        multi_class=\"ovr\",\n",
    "        average=\"micro\",\n",
    "    )\n",
    "\n",
    "    if show_stats:\n",
    "        print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{micro_roc_auc_ovr:.2f}\\n\")\n",
    "        print(f\"Test Set Accuracy : {accuracy_score(y_test, y_pred) * 100} %\\n\")\n",
    "        print(f\"Classification Report : \\n\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "\n",
    "    # saves auc score if desired\n",
    "    if save_auc: return micro_roc_auc_ovr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Support Vector Machine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ohhhhhh. Oh my gosh I'm so stupid\n",
    "# guys, we are we trying to use SVMs to classify more than two groups???\n",
    "# also no need for more than one fold (train test), due to data size and compute time\n",
    "# We can't use this unless we try splitting the groups ala one v. rest or KNN\n",
    "# trying again but with one v rest using SVM (rbf)\n",
    "\n",
    "# should take about 3.5 minutes for a 4 core, ~3ghz machine with fast RAM\n",
    "svm_clf = OneVsRestClassifier(estimator = SVC(probability = True, random_state = 1, kernel = 'rbf'), verbose = 3, n_jobs = 4)\n",
    "svm_clf.fit(X_train_bank, y_train_bank)\n",
    "svm_y_predict = svm_clf.predict(X_test_bank) # both predictions take around 3 min\n",
    "svm_y_prob = svm_clf.predict_proba(X_test_bank)\n",
    "\n",
    "svm_auc = metrics_classification_scorer(y_train_bank, y_test_bank, svm_y_predict, svm_y_prob, save_auc = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's try using a logit, max iterations needed to be raised due to terrible convergence rate\n",
    "# indicating this will not end with good test results...\n",
    "logit_fitted = LogisticRegression(max_iter = 1000).fit(X_train_bank, y_train_bank)\n",
    "logit_y_pred = logit_fitted.predict(X_test_bank)\n",
    "logit_y_prob = logit_fitted.predict_proba(X_test_bank)\n",
    "\n",
    "logit_auc = metrics_classification_scorer(y_train_bank, y_test_bank, logit_y_pred, logit_y_prob, save_auc = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. K-nearest Neighbours"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 10).fit(X_train_bank, y_train_bank)\n",
    "knn_y_pred = knn.predict(X_test_bank)\n",
    "knn_y_prob = knn.predict_proba(X_test_bank)\n",
    "knn_auc = metrics_classification_scorer(y_train_bank, y_test_bank, knn_y_pred, knn_y_prob, save_auc = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# possible to do a hyperparameter grid search using a loop such as...\n",
    "auc_list = []\n",
    "n = 500 # for now I had to cap it at 500. Computer falls asleep and python hangs so this is the most it can do unless\n",
    "# I run it on datahub/datacruncher\n",
    "max_auc = 0\n",
    "\n",
    "for k in list(range(1, n, 2)):\n",
    "    knn = KNeighborsClassifier(n_neighbors = k).fit(X_train_bank, y_train_bank)\n",
    "    knn_y_pred = knn.predict(X_test_bank)\n",
    "    knn_y_prob = knn.predict_proba(X_test_bank)\n",
    "    current_auc = metrics_classification_scorer(y_train_bank, y_test_bank, knn_y_pred, knn_y_prob, show_stats = False, save_auc = True)\n",
    "\n",
    "    if current_auc > max_auc:\n",
    "        max_auc = current_auc\n",
    "        best_knn = knn\n",
    "    if k % 50 == 0:\n",
    "        print(k ,\" of \", n)\n",
    "\n",
    "# for n = 500, k = 369 is the best with AUC of 0.59\n",
    "knn_grid_y_pred = best_knn.predict(X_test_bank)\n",
    "knn_grid_y_prob = best_knn.predict_proba(X_test_bank)\n",
    "\n",
    "metrics_classification_scorer(y_train_bank, y_test_bank, knn_grid_y_pred, knn_grid_y_prob, save_auc = False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Decision Tree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier().fit(X_train_bank, y_train_bank)\n",
    "tree_y_pred = tree.predict(X_test_bank)\n",
    "tree_y_prob = tree.predict_proba(X_test_bank)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree_auc = metrics_classification_scorer(y_train_bank, y_test_bank, tree_y_pred, tree_y_prob, save_auc = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Multi-layer Perceptron"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(random_state=42, max_iter=1000).fit(X_train_bank, y_train_bank)\n",
    "mlp_y_pred = mlp.predict(X_test_bank)\n",
    "mlp_y_prob = mlp.predict_proba(X_test_bank)\n",
    "mlp_auc = metrics_classification_scorer(y_train_bank, y_test_bank, mlp_y_pred, mlp_y_prob, save_auc = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4. Random Forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=42).fit(X_train_bank, y_train_bank)\n",
    "forest_y_pred = forest.predict(X_test_bank)\n",
    "forest_y_prob = forest.predict_proba(X_test_bank)\n",
    "forest_auc = metrics_classification_scorer(y_train_bank, y_test_bank, forest_y_pred, forest_y_prob, save_auc = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. Extra Trees"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# try an extra trees classifier (fits a number of randomized trees on various sub-samples of the dataset)\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etc = ExtraTreesClassifier(n_estimators=10000, random_state=42).fit(X_train_bank, y_train_bank)\n",
    "etc_y_pred = etc.predict(X_test_bank)\n",
    "etc_y_prob = etc.predict_proba(X_test_bank)\n",
    "etc_auc = metrics_classification_scorer(y_train_bank, y_test_bank, etc_y_pred, etc_y_prob, save_auc = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bank_coded.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.heatmap(bank_coded[['admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed',\n",
    "                        'services','student','technician','unemployed','unknown','divorced','married','single',\n",
    "                        'housing','loan','y','age','balance']].corr(),annot=True,linewidth=0.1,annot_kws={\"fontsize\":5})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "The main concern with prediction with real life data is the accuracies of our models. We are uncertain about the predictive power of the models and whether these models should be used to help with future marketing. One can misuse the models trained and infer possibly inaccurate conclusions on bank marketing.\n",
    "\n",
    "Furthermore, the data used are from a Portuguese banking institution and may not be similar with maketing data of banking institutions of other countries. The marketing data of other banking institutions in Portugal might also differ. The model created might not be accurate to predict marketing results of other banking institutions.\n",
    "\n",
    "Therefore, We are careful to disclose that the models we will generate are for reference only and we by no means guarantee they are accurate in predicting the market reception on banking. One needs to be cautious using the models trianed to predict patterns, generalize, and infer conclusions on bank marketing.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"moonnote\"></a>1.[^](#moon): Moon, Chris (14 Sept 2022) Average U.S. Checking Account Balance: A Demographic Breakdown. *ValuePenguin*. https://www.valuepenguin.com/banking/average-checking-account-balance.<br>\n",
    "<a name=\"socialnote\"></a>2.[^](#social): Social Security. Research, Statistics & Policy Analysis: Education and Lifetime Earnings. https://www.ssa.gov/policy/docs/research-summaries/education-earnings.html#:~:text=Men%20with%20bachelor's%20degrees%20earn,earnings%20than%20high%20school%20graduates.<br>\n",
    "<a name=\"relationshipnote\"></a>3.[^](#relationship): Social Security. Research, Statistics & Policy Analysis: The Relationship Between Retirement Savings and Marital Status Among Young Adults. https://www.ssa.gov/policy/docs/research-summaries/marital-status.html.<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
